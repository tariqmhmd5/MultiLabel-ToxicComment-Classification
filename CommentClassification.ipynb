{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk \n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "sys.getrecursionlimit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data and creating randomness\n",
    "x_train, x_test, y_train, y_test = train_test_split(train, train.iloc[:,-6:], shuffle = True,test_size=0.50, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing the data according to my ram size you may increase it accordint to your need\n",
    "#my pc is crashing and raising memory error if i use full data\n",
    "x_train = x_train[:5000]\n",
    "y_train = y_train[:5000]\n",
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.reset_index(inplace=True,drop=True)\n",
    "x_test.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8912ebe61e9f8429</td>\n",
       "      <td>we have to make three devisions for a general ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5fa590ea4d1d9041</td>\n",
       "      <td>\"\\nSpeedy deletion of Between the lines film\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fa9ca09121108ca5</td>\n",
       "      <td>\":Well, of course, since it's a piece of ficti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c7f11ec41819e516</td>\n",
       "      <td>I agree that the 20th century section is one s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d877e7d2d8198df2</td>\n",
       "      <td>Should this page exist? \\n\\nI grabbed this off...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  8912ebe61e9f8429  we have to make three devisions for a general ...      0   \n",
       "1  5fa590ea4d1d9041  \"\\nSpeedy deletion of Between the lines film\\n...      0   \n",
       "2  fa9ca09121108ca5  \":Well, of course, since it's a piece of ficti...      0   \n",
       "3  c7f11ec41819e516  I agree that the 20th century section is one s...      0   \n",
       "4  d877e7d2d8198df2  Should this page exist? \\n\\nI grabbed this off...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "for i,text in enumerate(x_train['comment_text']):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('[^A-Za-z\\' ]+', '',text) \n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    words = text.split()\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    x_train['comment_text'][i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8912ebe61e9f8429</td>\n",
       "      <td>make three devis gener pratimoksha articl ther...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5fa590ea4d1d9041</td>\n",
       "      <td>speedi delet line film tag place line film req...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fa9ca09121108ca5</td>\n",
       "      <td>well cours sinc piec fictici crap answer nobod...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c7f11ec41819e516</td>\n",
       "      <td>agre th centuri section one side seem blame in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d877e7d2d8198df2</td>\n",
       "      <td>page exist grab request articl page assum requ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  8912ebe61e9f8429  make three devis gener pratimoksha articl ther...      0   \n",
       "1  5fa590ea4d1d9041  speedi delet line film tag place line film req...      0   \n",
       "2  fa9ca09121108ca5  well cours sinc piec fictici crap answer nobod...      0   \n",
       "3  c7f11ec41819e516  agre th centuri section one side seem blame in...      0   \n",
       "4  d877e7d2d8198df2  page exist grab request articl page assum requ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "for i,text in enumerate(x_test['comment_text']):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('[^A-Za-z\\' ]+', '',text) \n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    words = text.split()\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    x_test['comment_text'][i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86e395fd449fac16</td>\n",
       "      <td>redirect name</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>da2d01ad7a9e23fb</td>\n",
       "      <td>sinebot pleas read comment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44d17de5e7064a55</td>\n",
       "      <td>thank good answer realiz read question stupid ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>681bceb39ce09938</td>\n",
       "      <td>think need ask like visit disambigu page matte...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8432afd4ddd2e1be</td>\n",
       "      <td>orangemonsterk svrtvdude vt april</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  86e395fd449fac16                                      redirect name      0   \n",
       "1  da2d01ad7a9e23fb                         sinebot pleas read comment      0   \n",
       "2  44d17de5e7064a55  thank good answer realiz read question stupid ...      0   \n",
       "3  681bceb39ce09938  think need ask like visit disambigu page matte...      0   \n",
       "4  8432afd4ddd2e1be                  orangemonsterk svrtvdude vt april      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if pc is not good this will couse memory error\n",
    "# ps = PorterStemmer()\n",
    "# def clean_text(text):\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r\"what's\", \"what is \", text)      ### conversion of contraction words to expanded words\n",
    "#     text = re.sub(r\"\\'s\", \" \", text)\n",
    "#     text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "#     text = re.sub(r\"can't\", \"can not \", text)\n",
    "#     text = re.sub(r\"n't\", \" not \", text)\n",
    "#     text = re.sub(r\"i'm\", \"i am \", text)\n",
    "#     text = re.sub(r\"\\'re\", \" are \", text)\n",
    "#     text = re.sub(r\"\\'d\", \" would \", text)\n",
    "#     text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "#     text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "#     text = re.sub('\\W', ' ', text)                                                 ### removing non-word characters\n",
    "#     text = re.sub('[^A-Za-z\\' ]+', '',text)                                        ### removing all non-alphanumeric values(Except single quotes)\n",
    "#     text = re.sub('\\s+', ' ', text)\n",
    "#     text = text.strip(' ')\n",
    "#     text = ' '.join([ps.stem(word) for word in text.split() if word not in (stop_words)])    ### Stopwords removal\n",
    "#     return text\n",
    "\n",
    "# x_train[\"comment_text\"] = train[\"comment_text\"].apply(clean_text)\n",
    "# # x_test[\"comment_text\"] = test[\"comment_text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words with ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2),max_features=3000)\n",
    "X = cv.fit_transform(x_train['comment_text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(5000,3000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data for testing+\n",
    "x_test = cv.transform(x_test['comment_text']).toarray()\n",
    "x_test = x_test.reshape(1000,3000,1)\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 3000, 64)          16896     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 50,310\n",
      "Trainable params: 50,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units = 64, dropout = 0.2,input_shape=(3000,1),return_sequences=True))\n",
    "model.add(LSTM(units = 64, dropout = 0.2))\n",
    "model.add(Dense(units = 6, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1207s 8s/step - loss: 0.1929 - auc: 0.6745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f60dc67c88>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"AUC\"])\n",
    "model.fit(X, Y, batch_size = 32, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11848846, 0.01138544, 0.06097275, 0.00398451, 0.06229734,\n",
       "        0.0130434 ],\n",
       "       [0.11848855, 0.01138544, 0.06097275, 0.00398448, 0.06229734,\n",
       "        0.0130434 ],\n",
       "       [0.11848855, 0.01138544, 0.06097275, 0.00398451, 0.06229728,\n",
       "        0.0130434 ],\n",
       "       ...,\n",
       "       [0.11848846, 0.01138544, 0.06097275, 0.00398451, 0.06229734,\n",
       "        0.0130434 ],\n",
       "       [0.11848855, 0.01138544, 0.06097275, 0.00398451, 0.06229728,\n",
       "        0.0130434 ],\n",
       "       [0.11848855, 0.01138544, 0.06097275, 0.00398451, 0.06229728,\n",
       "        0.0130434 ]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the output of y_pred is producing the 6 element array which is the probability of belong to particular label\n",
    "# In this file we used bag of words with n grams range = 2 \n",
    "# train test split is used to divide the dataset with some randomness\n",
    "# the reasono i choose less data is because my not have much memory to handle that amount of data\n",
    "# you may use full data if your pc is good enought to handle the data..\n",
    "# for further we can use word embeddings to improve the model (word2vec,embedding layer etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Dense(16, input_dim=3000, activation='relu'))\n",
    "nn.add(Dense(12, activation='relu'))\n",
    "nn.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                48016     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                204       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 78        \n",
      "=================================================================\n",
      "Total params: 48,298\n",
      "Trainable params: 48,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(5000,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1/157 [..............................] - ETA: 0s - loss: 0.1125 - accuracy: 0.3438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\deeplearning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3350: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 3s 19ms/step - loss: 0.3898 - accuracy: 0.9288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f6063db288>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "nn.fit(X, Y, batch_size = 32, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reshape(1000,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\deeplearning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3350: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
     ]
    }
   ],
   "source": [
    "pred_nn = nn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28308025, 0.09725911, 0.22343646, 0.08308864, 0.17477451,\n",
       "        0.13836104],\n",
       "       [0.29106665, 0.0940033 , 0.2262557 , 0.08221618, 0.17193589,\n",
       "        0.13452226],\n",
       "       [0.29032215, 0.0927763 , 0.22921862, 0.07814758, 0.17338315,\n",
       "        0.13615228],\n",
       "       ...,\n",
       "       [0.28308025, 0.09725911, 0.22343646, 0.08308864, 0.17477451,\n",
       "        0.13836104],\n",
       "       [0.27722937, 0.1001443 , 0.22136518, 0.0869699 , 0.17356169,\n",
       "        0.1407296 ],\n",
       "       [0.28605756, 0.09687138, 0.22404775, 0.07975716, 0.17465068,\n",
       "        0.1386155 ]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question and Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. Experimentation with different experiment setups:\n",
    "\n",
    "a. data pre-processing techniques – tokenise (e.g. will you use n-grams?), normalise text, apply stopwords, and so on\n",
    "\n",
    "Answer: For preprocessing first the regular expression is used to remove the special characters and symbols from the dataset then stopwords is applied to remove the unnessary words, after that the texxt is now ready for vectorization for this bag of word technique is used with bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. NLP algorithms and techniques – explain choice\n",
    "\n",
    "Answer: For classification of text first we experiment with LSTM model which is widely known for text classsification as it stores the memory state after it a simple neural network is used for classification which is very accurate. As after applying the bag of words (countvectorizer) the input feature become 3000 long so these two model are best for this much feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. text featurisation/transformation into numerical vectors – justify choices like one hot encoding, and other relevant methods\n",
    "\n",
    "Answer: the one hot encoding is not possible for this amount of data because we have huge corpus and using one hot vectors is not efficient in for this task. so we used bag of word for vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. training/text/validate dataset splitting – how did you split and why\n",
    "\n",
    "Answer: for this we use train_test_split from sklearn which provide randomness in the data after splitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. choices of loss functions and optimisers (if appropriate/relevant) – explain your choices with facts from the results\n",
    "\n",
    "Answer: As our problem is multi label classification problem so it is obvious to use categorical_crossentropy loss function because it is always used for this type of tassks and adam optimizer is best because it finds the convergence in very effictive manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
